 			+--------------------+
			|        CS 140      |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Michael Malek <mjmalek218@protonmail.com>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


NEW VARIABLE DECLARATION: "struct list timer_sleep_list;"

PURPOSE: This declaration was added to thread.h. It is a global list
	 meant to keep track of all threads that have put themselves to sleep. 

NEW VARIABLE DECLARATION: "struct thread* sleeping_thread"

PURPOSE: Added to thread.c. Simply a handle to the current sleeping thread in
the iteration over timer_sleep_list.

NEW VARIABLE DECLARATION: "struct list_elem* e"

PURPOSE: Added to thread.c, just used to iterate over timer_sleep_list when 
updating sleep_ticks of each sleeping thread. 

CHANGED STRUCT: (note: only the added field is shown in the struct


	"struct thread
	{
	        ...everything else is kept. the addition is noted below:

		/* The number of ticks left for the thread to sleep, when it
		puts itself to sleep with a timer_sleep call. reset
		at every timer_sleep call. */
		int64_t sleep_ticks;
	};"

PURPOSE:


---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

First interrupts are disabled. Then the current thread's struct
is pushed onto a global list of sleeping threads, and a field I added to each thread 
struct, called sleep_ticks (described above), is set to the number of ticks remaining
(on first iteration, this is just the argument "ticks" to the function, unless
a timer interrupt occurs between the intialization of "start" and the disabling
of interrupts).  Finally the thread is blocked. When the thread is eventually
unblocked by the timer_handler, it is put on the ready queue. When it runs it puts
interrupts at the level they were prior to calling timer_sleep, and continues execution.

The reason this occurs in a while loop is because I'm afraid that, if the scheduler
were for to some reason wake the thread that put itself to sleep up, it could 
potentially sleep for less time than it is supposed to. The way things are, the 
thread is guaranteed to be disabled for at least the number of ticks it wanted to
be. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

What are the additional steps that *must* occur in the timer interrupt
handler?

	-- All sleep ticks for all sleeping threads *must* be decremented
	   and then checked for zero

	-- If one reaches 0, remove the sleeping thread from the list of 
	   threads that have put themselves to sleep, and *unblock* the thread
	   (i.e. don't run it just put it on the ready queue)

Thus, the best way we could minimize time spent in the timer interrupt handler
is *only* adding the above steps to the handler, and performing them efficiently,
which is what we did.  

One way I could have potentially made this faster was by
maintaining simply some sort of array of the ticks, and rather than iterating over
points in a list object we could simply just iterate through the array
decrementing each amount of ticks, but I didn't think the added complexity
of allocating separate memory for the array and re-sizing this memory every now
and then would be worth the amoritzed performance benefits. 

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

At my first shot at implementation of timer_sleep(), I attempted to use
synchronization primitives (lock + condition variable to signal to a sleeping thread
to wake up). I realized, however, that synchronization of access to the timer_sleep_list
needs to occur not only amongst threads putting themselves to sleep, but also the timer    
interrupt handler, which cannot be controlled by locks. 

Thus, to answer the question, I just disabled interrupts in timer_sleep.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Again, the only way that I could think to do this was by disabling interrupts, since
the interrupt handler cannot sleep/employ synchronization primitives. 

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

The design above was the simplest, bare-bone design I could think of. As discussed
in several of the answers above, there were two alterations I considered:

   1. Using locks to control access to the timer_sleep list, and 

   2. Using an array of timer ticks remaining for each thread, and a linked list
   of struct threads, as opposed to putting the ticks remaining as a field in the 
   struct threads, so we could operatore primarily out of an array and use a linked
   list only when necessary (the former being faster)

In the case of (1.), the idea simply was not feasible as explained in A.4. In the case
of (2.), the increased complexity of attempting to implement such a solution for  
a marginal amortized performance benefit was simply not worth it (also explained in A.3),
as it would have required allocating+dynamically resizing the array of ticks. 


			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

*************** BEGIN FIRST CHANGE: struct thread ************************

struct thread
  {
    /* Owned by thread.c. */
    tid_t tid;                          /* Thread identifier. */
    enum thread_status status;          /* Thread state. */
    char name[16];                      /* Name (for debugging purposes). */
    uint8_t *stack;                     /* Saved stack pointer. */

    struct list_elem allelem;           /* List element for all threads list. */

    /* Shared between thread.c and synch.c. */
    struct list_elem elem;              /* List element. */

#ifdef USERPROG
    /* Owned by userprog/process.c. */
    uint32_t *pagedir;                  /* Page directory. */
#endif

    /* Owned by thread.c. */
    unsigned magic;                     /* Detects stack overflow. */

    /* BEGIN FIELDS I HAVE ADDED FOR THE MASTER BRANCH */
    
    /* general purpose synchronization tool

       COMMENTING THIS OUT. I HAVE NO IDEA WHY I PUT THIS HERE
*/  
    //struct condition cond; 
    
    /* The number of ticks left for the thread to sleep, when it
       puts itself to sleep with a timer_sleep call. reset
       at every timer_sleep call. */
    int64_t sleep_ticks;

    /* END FIELDS I HAVE ADDED FOR THE MASTER BRANCH */


    /* BEGIN FIELDS I HAVE ADDED FOR THE PRIORITY SCHEDULING BRANCH */

    /* This list will constitute *all* locks held by the thread currently */
    struct list locks_held;

    /* Lock the thread is waiting on. NULL if blocked on no lock. */
    struct lock* waiting_on;
    
    /* The maximum of all donated priorities and the thread's native priority. */
    int merged_priority;
    int native_priority;                /* reg Priority with new name */
    
    /* END FIELDS I HAVE ADDED FOR THE PRIORITY SCHEDULING BRANCH */
  };

*************** struct thread DISCUSSION ************************

Added:

     -- "sleep_ticks" field: when a thread puts itself to sleep, this field
       is updated at every timer tick until it reaches 0, and the thread
       is unblocked
     -- "locks_held" field: list of locks a thread is holding. helps in two
        ways:

        1. When "thread_set_priority()" is called, allows the "merged_priority"
	   field to be accurately updated. 

        2. When "lock_release()" is called, allows the "merged_priority"
	   field to be accurately updated. 

     -- "waiting_on" field: the lock a thread may be waiting on. Necessary
     	for chaining and allowing a high priority thread to donate through
	a list of threads waiting on different locks (mainly used in the
	"percolate_priority(...)" function)

     -- "merged_priority" field: the max of all donated priorities, and the
     	native priority. If the thread is holding no locks, merged = native

Changed:

     --"priority" to "donated_priority": reduces ambiguity with the new
        "merged_priority" field


*************** END FIRST CHANGE: struct thread ************************

*************** BEGIN SECOND CHANGE: struct lock ************************


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)






---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

In the Pintos base code, locks are built upon semaphores, so altering the code
so that in a semaphore locks are woken up first naturally changes how locks
function as well.

As for condition variables, the original code was written as follows:

   1. Have a condition variable consist only of a list (representing the threads waiting) 

   2. Whenever a thread waits on a condition, create a corresponding *semaphore_elem*,
      which is really just a semaphore and a list element wrapped into one struct,
      so that it may be placed on the condition waiters list. 
      Thus, each single thread waiting on the condition has its own semaphore_elem->elem
      holding its place in the list. 

   3. The semaphore component of the semaphore_elem is initialized to 0. Again list_elem
      is not used (unless a timer interrupt occurs and the thread gets de-scheduled, in
      which case the thread's list_elem is placed on the ready list, but this really isn't
      a problem since we only intend to use the native synch elem once the thread blocks
      later)

   4. Once it is initialized to 0, the semaphore_elem's list_elem is placed on the cond_wait
      list. Again no use of the native list_elem

   5. Immediately after the lock is released, and the semaphore is downed. *This* is where
      the list_elem is used, and in fact, the fact that it is used here assures me that
      replacing this method with simply pushing the thread's original list_elem will not
      interfere with other conflicting use of the native list_elem that I am not currently
      aware of.

   6. The ordering of the steps above is important: list push back, lock release, sema_down.
      If the lock were released before the sema were downed, then theres a chance the thread
      sleeps forever when it reaches sema_down. If the semaphore were downed before lock_release,
      the thread would sleep while holding the lock, and so it would sleep forever and block
      any use of the condition variable.


There are two issues I personally have with this implementation. Firstly,
there is no easy way to access waiting threads' priorities. Secondly, it is needlessly
complex: can just place the struct list_elem* elem already provided there. 


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

void lock_acquire(struct lock* lock), as of right now, goes through the following steps:

     0. if the lock isn't held, just acquire it and *update* the lock's struct to indicate
     	that the current thread is holding it (new struct don't forget this)

     1. If the lock is held by another thread, update the thread's struct to indicate
     	it is waiting on that lock

     2. Call percolate_priority(int merged_priority, struct lock* lock), which does
     	the following:

     	a. checks to see if the thread holding the lock has higher merged_priority
	   than the inputted

	b. if yes, finishes

	c. if no, updates the "merged_priority" and "donated_priority" fields

	d. checks to see if the thread holding the lock is waiting on another thread

	e. if yes, calls itself recursively on the new lock *with the old priority*

	f. if no, returns.


	NOTE: percolate_priority *must* be called with interrupts disabled

     3. once percolate_priority returns, the thread blocks. 


>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.


     1. the locks's "holder" field is updated accordingly

     2. the thread releasing the lock calls a helper function,
     	"update_priority" which does the following:

	a. 

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

Example race:

   1. thread_set_priority() is called to set a high priority

   2. native_priority is set correctly but then a timer interrupt occurs,
      so merged is still old value

   3. // TODO TODO TODO, need to think about this more. 

   4. 

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0
 4
 8
12
16
20
24
28
32
36

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

